<!DOCTYPE html>
<!-- saved from url=(0070)http://reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>How to train your own model for CoreML</title>

  <!-- CSS -->
  <link rel="stylesheet" href="./How to train your own model for CoreML_files/main.css" type="text/css">

  <!-- Font -->
  <link rel="stylesheet" href="./How to train your own model for CoreML_files/font-awesome.min.css">
  <link href="./How to train your own model for CoreML_files/SpoqaHanSans-kr.css" rel="stylesheet" type="text/css">
  <link href="./How to train your own model for CoreML_files/css" rel="stylesheet">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Reza Shirazian" href="http://reza.codes/feed.xml">
  <!-- Begin Jekyll SEO tag v2.2.3 -->
<title>How to train your own model for CoreML | Reza Shirazian</title>
<meta property="og:title" content="How to train your own model for CoreML">
<meta property="og:locale" content="en_US">
<meta name="description" content="A guide to setup, train and integrate your own CoreML model.">
<meta property="og:description" content="A guide to setup, train and integrate your own CoreML model.">
<link rel="canonical" href="http://www.reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/">
<meta property="og:url" content="http://www.reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/">
<meta property="og:site_name" content="Reza Shirazian">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-07-29T00:00:00-07:00">
<script async="" src="./How to train your own model for CoreML_files/analytics.js"></script><script type="application/ld+json">
{"@context":"http://schema.org","@type":"BlogPosting","headline":"How to train your own model for CoreML","author":{"@type":"Person","name":null},"datePublished":"2017-07-29T00:00:00-07:00","dateModified":"2017-07-29T00:00:00-07:00","description":"A guide to setup, train and integrate your own CoreML model.","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/"},"url":"http://www.reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/"}</script>
<!-- End Jekyll SEO tag -->



  <!-- Google Analytics -->

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-62822211-1', 'auto');
ga('send', 'pageview');

</script>



<script src="./How to train your own model for CoreML_files/embed.js" data-timestamp="1501960588006"></script><link rel="preload" as="style" href="https://c.disquscdn.com/next/embed/styles/lounge.eceee602870fc4ed49dc5f89e270689e.css"><link rel="preload" as="script" href="https://c.disquscdn.com/next/embed/common.bundle.9fcff11af667507b1757062f0192b821.js"><link rel="preload" as="script" href="https://c.disquscdn.com/next/embed/lounge.bundle.aafec1a2f3fa1e486216be04908b0e3a.js"><link rel="preload" as="script" href="https://disqus.com/next/config.js"><script src="./How to train your own model for CoreML_files/alfie.f51946af45e0b561c60f768335c9eb79.js" async="" charset="UTF-8"></script></head>

<body>
  <div class="content-container">
    <header>
  <div class="header-small">
    <a href="http://www.reza.codes/">Reza Shirazian</a>
  </div>
</header>
<div class="post">
  <div class="post-title">How to train your own model for CoreML</div>
  <span class="post-date">
    <time>29 Jul 2017</time>
  </span>
  <div class="post-tag">
    <ul>
      
    </ul>
  </div>

  <p>In this guide we will train a <a href="http://reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/#caffe">Caffe</a> model using<a href="http://reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/#digits"> DIGITS</a> on an <a href="http://reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/#ec2">EC2</a> <code class="highlighter-rouge">g2.2xlarge</code> instance, convert it into a <a href="http://reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/#coreml">CoreML</a> model using Apple’s <a href="http://reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/#coremltool">coremltools</a> and integrate it into an iOS app. If some of what mentioned is unfamiliar or sounds intimidating, fear not I will try to cover the process inasmuch detail as possible.  we will do all of this while building SeeFood, the app <a href="https://www.youtube.com/watch?v=FNyi3nAuLb0">first introduced on HBO’s popular TV show Silicon Valley</a>. The difference is that we will succeed where Elrich and Jing Yang failed and go beyond Hot Dog/Not Hot Dog. By the end of this guide we will have trained our own model and integrated it in an iOS app that will detect from 101 dishes locally on the device.</p>

<div style="margin: 0 auto;width:320px;">
<video width="100%" autoplay="" controls="" align="center">
  <source src="https://s3.amazonaws.com/pixpit/See+Food-+Detect+various+dishes+using+CoreML.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</div>

<p>This guide is broken down into three parts:</p>
<ul>
<li>  <a href="http://reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/#train"> Train</a>  </li>
<li> <a href="http://reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/#convert"> Convert</a> </li>
<li>  <a href="http://reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/#integrate"> Integrate</a> </li>
</ul>

<p>Each part builds on the part before it, however I will provide all the resources needed to pick up and continue from anywhere.</p>

<p>Just like most things in life there are many ways to achieve the same goal. This guide covers one of many ways to train, convert and integrate an image classifier into an iOS app. If any of the decisions made in this guide does not suite you, I encourage you to explore and find what works best for you.</p>

<p>If you have suggestions or want to reach out to me directly, feel free to do so through Twitter:  <a href="https://twitter.com/kingreza">Reza Shirazian</a></p>

<p>There are a set of prerequisites that you need  before continuing with this guide:</p>
<ul>
<li>An AWS account with enough credit to run an g2.2xlarge for a 6-7 hours</li>
<li>Xcode 9 (Version 9.0 beta 3 (9M174d))</li>
<li>An iOS device running iOS 11 (currently in beta)</li>
<li>A basic understanding of programming</li>
</ul>

<p>If you’re all set we can begin</p>

<p><a name="train"></a></p>
<h2>Part 1: Train </h2>

<p>This part can technically be split into two different parts: <a href="http://reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/#setup">Setup</a> and <a href="http://reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/#training">Training</a>. First we’ll cover setting up an EC2 instance and the then we’ll focus on the actual training. I will highlight the process for setting up the instance in more details since it’s something that might be a bit more foreign to iOS developers. If you’re an expert with AWS then here is the summary of the setup:</p>

<p>We will launch a preconfigured amazon machine image (AMI) that will have Caffe and NVIDIA’s DIGITS already set up. This image was created by <a href="https://www.learnopencv.com/about/">Satya Mallick</a> and is named “bigvision-digits” under public images.  I encourage you to check out his <a href="https://www.learnopencv.com/computer-vision-machine-learning-artificial-intelligence-consulting/">website</a> and view his <a href="https://www.youtube.com/watch?v=QZaAcl_F9R0">youtube tutorial</a> which will have a lot of overlap with what’s highlighted in this guide. We will use a <code class="highlighter-rouge">g2.2xlarge</code> instance to take advantage of GPU computing for more efficient training. This instance type is not free and at the time of writing costs about $0.65/hour.  For more up-to-date pricing check out <a href="https://aws.amazon.com/ec2/pricing/on-demand/">Amazon instance pricing</a>. Fortunately you won’t need to run the instance for more than 6-7 hours, but do make sure to <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html">stop it</a> when you’re done. Running one of these for a whole month can cost  close to $500.</p>

<p><a name="setup"></a></p>
<h3> Setup </h3>

<p>Sign into your AWS console and click on EC2  <br>
<img src="./How to train your own model for CoreML_files/1.jpg" width="100%" style="margin: 0 auto"></p>

<p>Select AMIs on the left hand side 
<img src="./How to train your own model for CoreML_files/2.jpg" width="100%" style="margin: 0 auto"></p>

<p>Select Public Images from the dropdown next to the search bar
<img src="./How to train your own model for CoreML_files/3.jpg" width="100%" style="margin: 0 auto"></p>

<p>Search for <code class="highlighter-rouge">bigvision-digits</code>, select the image and click launch
<img src="./How to train your own model for CoreML_files/4.jpg" width="100%" style="margin: 0 auto"></p>

<p>Select <code class="highlighter-rouge">g2.2xlarge</code> from the list of instance types, click “Next: Configure Instance Details”
<img src="./How to train your own model for CoreML_files/5.jpg" width="100%" style="margin: 0 auto"></p>

<p>There is no need to change anything here, Click on “Add Storage”
<img src="./How to train your own model for CoreML_files/6.jpg" width="100%" style="margin: 0 auto"></p>

<p>Change size to 75 Gib and click “Add Tags”
<img src="./How to train your own model for CoreML_files/7.jpg" width="100%" style="margin: 0 auto"></p>

<p>Add Key “Name” and set its value to something descriptive. In this example I picked “Caffe DIGITS”. Click on “Configure Security Group”
<img src="./How to train your own model for CoreML_files/8.jpg" width="100%" style="margin: 0 auto"></p>

<p>Click Add Rule, select Custom TCP with protocol TCP, Port Range 80.   <br>
Set Source for SSH and Custom TCP to My IP.    <br>
Click “Review and Launch”.   <br>
<img src="./How to train your own model for CoreML_files/9.jpg" width="100%" style="margin: 0 auto"></p>

<p>Review everything and click “Launch”
<img src="./How to train your own model for CoreML_files/10.jpg" width="100%" style="margin: 0 auto"></p>

<p>Select “Create a new key pair” and use “digits” as key pair name. Click “Download Key Pair”
This will download a file named <code class="highlighter-rouge">digits.pem</code>. Remember where this file is and make sure you don’t lose it. You will need it to access your instance later.
Click “Launch Instances”.
<img src="./How to train your own model for CoreML_files/11.jpg" width="100%" style="margin: 0 auto"></p>

<p>Wait for a few minutes while the instance gets setup. Click on your instance and look at its description. Once the Instance State is set to running and you can see an IP value by “IPv4 Public IP” your EC2 is ready. 
<img src="./How to train your own model for CoreML_files/12.jpg" width="100%" style="margin: 0 auto"></p>

<p>Copy and past the public IP into your browser and you should see the following
<img src="./How to train your own model for CoreML_files/13.jpg" width="100%" style="margin: 0 auto"></p>

<p>DIGITS is up and running.
Before we can begin using our instance and DIGITS to train our model we need a dataset. We will be using Food 101 dataset which can be <a href="https://www.vision.ee.ethz.ch/datasets_extra/food-101/">found here</a> . To download the dataset into our instance we need to SSH into it. Open your terminal and follow along</p>

<p><img src="./How to train your own model for CoreML_files/14.jpg" width="100%" style="margin: 0 auto">
Navigate to the folder where you downloaded <code class="highlighter-rouge">digits.pem</code> and change the file’s access permission to 600 (for more info on what this means <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html">click here</a>):</p>

<p><code class="highlighter-rouge">chmod 600 digits.pem</code></p>

<p>SSH into your instance by running the following command, replace YOUR INSTANCE’S PUBLIC ADDRESS with your instance’s public address. The very same IP you pasted into your browser to view DIGITS.</p>

<p><code class="highlighter-rouge">ssh -i digits.pem ubuntu@[YOUR INSTANCE'S PUBLIC ADDRESS]</code></p>

<p>After a few seconds you should be connected to your instance. If you’re unfamiliar with with the terminal fear not the next few steps are fairly straightforward.</p>

<p>View the folders available to you.</p>

<p><code class="highlighter-rouge">ls</code></p>

<p>Go to the data folder</p>

<p><code class="highlighter-rouge">cd data</code></p>

<p>In here you will see a folder named <code class="highlighter-rouge">17flowers</code> which is part of<a href="https://www.youtube.com/watch?v=QZaAcl_F9R0"> Satya Mallick’s tutorial</a>. He uses this dataset to train a flower classifier. We however will use a different dataset to train a different model.</p>

<p>Create a new folder named 101food</p>

<p><code class="highlighter-rouge">mkdir 101food</code></p>

<p>Navigate into the folder.</p>

<p><code class="highlighter-rouge">cd 101food</code></p>

<p>Download the dataset:</p>

<p><code class="highlighter-rouge">wget "http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz"</code></p>

<p>This process will take awhile. This dataset is more than 5GB is size. Once the file is downloaded unzip it using the following command</p>

<p><code class="highlighter-rouge">tar -xzf food-101.tar.gz</code></p>

<p>This too will take a few minutes. Once done there should be a <code class="highlighter-rouge">food-101</code> folder. Navigate to its ` images`  folder and you should see a folder for each dish type we will classify.
<img src="./How to train your own model for CoreML_files/15.jpg" width="100%" style="margin: 0 auto"></p>

<p>Exit out of the terminal and go back to your browser.</p>

<p><a name="training"></a></p>
<h3> Training</h3>
<p>Navigate to your instance’s public IP. Under “Datasets” selects “images” and then “classification”.</p>

<p><img src="./How to train your own model for CoreML_files/16.jpg" width="100%" style="margin: 0 auto"></p>

<p>Change “Resize Transformation” to “Crop”, select the folder for food-101 images as the “Training Images”. If you’ve followed this guide your folder will be <code class="highlighter-rouge">/home/ubuntu/data/101food/food-101/images</code>. Set “Dataset Name”to <code class="highlighter-rouge">101food</code> and click create
<img src="./How to train your own model for CoreML_files/17.jpg" width="100%" style="margin: 0 auto"></p>

<p>DIGITS will begin to create a database based off your dataset. This database will be used by Caffe during training. 
<img src="./How to train your own model for CoreML_files/18.jpg" width="100%" style="margin: 0 auto">
<img src="./How to train your own model for CoreML_files/19.jpg" width="100%" style="margin: 0 auto">
<img src="./How to train your own model for CoreML_files/20.jpg" width="100%" style="margin: 0 auto"></p>

<p>When the process is complete you should be able to explore your database. With the database ready we are now ready to train our model. If you ever use your own dataset, it’s worth knowing that DIGITS doesn’t require any specific mapping or label file. It will create the database based off the folder structure. 
<img src="./How to train your own model for CoreML_files/21.jpg" width="100%" style="margin: 0 auto"></p>

<p>Go back home on your DIGITS instance. On the “Models” panel select the “Images” dropdown and pick “Classifications”.
<img src="./How to train your own model for CoreML_files/22.jpg" width="100%" style="margin: 0 auto"></p>

<p>On the “New Image Classification Model” page under “Select Dataset” select “101food”. Under “Standard Networks” tab select “Caffe” and pick “AlexNet” as your model. Next select “Customize.”
<img src="./How to train your own model for CoreML_files/23.jpg" width="100%" style="margin: 0 auto"></p>

<p>Technically you could start training your model now but to get better accuracy we are going to use <a href="http://reza.codes/2017-07-29/how-to-train-your-own-dataset-for-coreml/#alexnet">AlexNet</a> and fine-tune existing weights from the original trained model with our dataset. To do this we need to select the original pretrained model which is available with Caffe. Under “Pretrained model” type <code class="highlighter-rouge">/home/ubuntu/models/bvlc_alexnet.caffemodel</code>. (DIGITS will provide autocompletion so finding the original model should be easy)</p>

<p>Next we need to change the “Base Learning Rate”. Since this model is already trained we need our dataset to retrain its weights at a smaller rate. So change “Base Learning Rate” from <code class="highlighter-rouge">0.01</code> to <code class="highlighter-rouge">0.001</code>.</p>

<p>Next we need to change the name of the final layer on our network. The pretrained model for AlexNet was trained to classify 1000 objects, our dataset classifies 101. This can be done by simply changing <code class="highlighter-rouge">fc8</code> to something else. In this example we’ll be changing it to <code class="highlighter-rouge">fc8_food</code>. So under “Customize Networks” change all instances of <code class="highlighter-rouge">fc8</code> to <code class="highlighter-rouge">fc8_food</code>.   <br>
<img src="./How to train your own model for CoreML_files/24.jpg" width="100%" style="margin: 0 auto"></p>

<p>Visualize the model to ensure that the last layer has successfully been renamed to “fc8_food”.
<img src="./How to train your own model for CoreML_files/25.jpg" width="100%" style="margin: 0 auto"></p>

<p>For a more indepth explanation of this process check out Flickr’s fine tuning of CaffeNet: <a href="http://caffe.berkeleyvision.org/gathered/examples/finetune_flickr_style.html">Fine-tuning CaffeNet for Style Recognition on “Flickr Style” Data</a></p>

<p>Name your model and click create.</p>

<p>DIGITS will begin to train your model. This process can take some time. For me, with a <code class="highlighter-rouge">g2.2xlarge</code> instance it took about four hours. 
<img src="./How to train your own model for CoreML_files/26.jpg" width="100%" style="margin: 0 auto"></p>

<p>Once the process is complete, we’ll see that our model can detect various dishes at a 65% success rate. This is ok but not perfect. To get a better accuracy, you would need a larger dataset which is something you can take on, but for now we’re going to settle for the 65% and proceed.
<img src="./How to train your own model for CoreML_files/27.jpg" width="100%" style="margin: 0 auto"></p>

<p>At the bottom upload or use a URL of an image to test the trained model. Do this multiple times with images that were not part of the training set. Check “Show visualizations and statistics” to see how each layer processed the image. 
<img src="./How to train your own model for CoreML_files/28.jpg" width="100%" style="margin: 0 auto"></p>

<p><img src="./How to train your own model for CoreML_files/29.jpg" width="100%" style="margin: 0 auto">
<img src="./How to train your own model for CoreML_files/30.jpg" width="100%" style="margin: 0 auto">
The model is trained and ready to be converted to CoreML. Click “Download Model”.</p>

<p><a href="https://s3.amazonaws.com/pixpit/coreml/trained_model.tar.gz">Click here to download the trained model</a></p>

<p><a name="convert"></a></p>
<h2>Part 2: Convert </h2>

<p>To convert an existing model to CoreML’s <code class="highlighter-rouge">.mlmodel</code> we need to install <code class="highlighter-rouge">coremltools</code>. Check out <a href="https://pypi.python.org/pypi/coremltools">https://pypi.python.org/pypi/coremltools</a> to familiarize yourself with the library.</p>

<p><img src="./How to train your own model for CoreML_files/31.jpg" width="100%" style="margin: 0 auto"></p>

<p>To install coremltools run from the terminal</p>

<p><code class="highlighter-rouge">pip install -U coremltools</code></p>

<p>coremltools requires Python 2.7 If you get <em>Could not find a version that satisfies the requirement coremltools (from versions: )</em> error it’s likely that you’re not running Python 2.7. If you don’t get this error and everything works  then you’re luckier than I was and can skip the next few steps.</p>

<p><img src="./How to train your own model for CoreML_files/32.jpg" width="100%" style="margin: 0 auto"></p>

<p>One way to solve this is to setup a virtual environment with Python 2.7.  You can do this by using Anaconda Navigator which can be downloaded along with <a href="https://www.continuum.io/downloads">Anaconda here</a></p>

<p>Once you have Anaconda Navigator running click on “Environments” on the left hand side. 
<img src="./How to train your own model for CoreML_files/33.jpg" width="100%" style="margin: 0 auto"></p>

<p>And select “Create” under the list of environments.
<img src="./How to train your own model for CoreML_files/34.jpg" width="100%" style="margin: 0 auto"></p>

<p>Type “coreml” for name, check Python and  select “2.7” from the dropdown. Click create.
<img src="./How to train your own model for CoreML_files/35.jpg" width="100%" style="margin: 0 auto"></p>

<p>You should have the new “coreml” environment up and running after a few second. Click on the new environment and press play. A new command line should open up.
<img src="./How to train your own model for CoreML_files/36.jpg" width="100%" style="margin: 0 auto"></p>

<p>If you run <code class="highlighter-rouge">pythons --version</code> you should see that your new environment is now running under python 2.7</p>

<p>Run <code class="highlighter-rouge">pip install -U coremltools</code> from your new command line. If you see a <em>no file or directory</em> error for any of the packages run  <code class="highlighter-rouge">pip install -U coremltools --ignore-installed</code>
<img src="./How to train your own model for CoreML_files/38.jpg" width="100%" style="margin: 0 auto"></p>

<p>Navigate to the folder where you downloaded the trained model. Create a new file named <code class="highlighter-rouge">run.py</code> and write the following code.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">coremltools</span>

<span class="c"># Convert a caffe model to a classifier in Core ML</span>
<span class="n">coreml_model</span> <span class="o">=</span> <span class="n">coremltools</span><span class="o">.</span><span class="n">converters</span><span class="o">.</span><span class="n">caffe</span><span class="o">.</span><span class="n">convert</span><span class="p">((</span><span class="s">'snapshot_iter_24240.caffemodel'</span><span class="p">,</span>
													 <span class="s">'deploy.prototxt'</span><span class="p">,</span>
													 <span class="s">'mean.binaryproto'</span><span class="p">),</span>
													  <span class="n">image_input_names</span> <span class="o">=</span> <span class="s">'data'</span><span class="p">,</span>
													  <span class="n">class_labels</span> <span class="o">=</span> <span class="s">'labels.txt'</span><span class="p">,</span>
													  <span class="n">is_bgr</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">image_scale</span><span class="o">=</span><span class="mf">255.</span><span class="p">)</span>

<span class="c"># Now save the model</span>
<span class="n">coreml_model</span><span class="o">.</span><span class="n">author</span> <span class="o">=</span> <span class="s">"Reza Shirazian"</span>
<span class="n">coreml_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">'food.mlmodel'</span><span class="p">)</span>
</code></pre>
</div>

<p>The first line imports<code class="highlighter-rouge"> coremltools</code>. Then we create a <code class="highlighter-rouge">coreml_model</code> and provide all the necessary input for coremltool to convert the Caffe model into a <code class="highlighter-rouge">.mlmodel</code>. The references to the files passed as the parameter works as is if <code class="highlighter-rouge">run.py</code> is in the same folder as the unpacked trained model downloaded earlier from DIGITS.</p>

<p>You can provide other metadata such as author, description and licence for the converted model. For more details I suggest going over coremltools documentation here: <a href="https://pythonhosted.org/coremltools/">https://pythonhosted.org/coremltools/</a></p>

<p><img src="./How to train your own model for CoreML_files/40.jpg" width="100%" style="margin: 0 auto"></p>

<p>Save <code class="highlighter-rouge">run.py</code> and run it from the console:</p>

<p><code class="highlighter-rouge">Python run.by</code></p>

<p><img src="./How to train your own model for CoreML_files/41.jpg" width="100%" style="margin: 0 auto"></p>

<p>This process will take a few minutes. Once completed you will have a <code class="highlighter-rouge">food.mlmodel</code> in the same folder as <code class="highlighter-rouge">run.py</code>. You’re now ready to integrate your trained CoreML model into an iOS project.</p>

<p><img src="./How to train your own model for CoreML_files/42.jpg" width="100%" style="margin: 0 auto">
<a name="integrate"></a></p>

<p><a href="https://s3.amazonaws.com/pixpit/coreml/food.mlmodel">Click here to download the CoreML model</a></p>
<h2>Part 3: Integrate </h2>

<p>Start a new Xcode project. Select iOS and Single View App.
<img src="./How to train your own model for CoreML_files/43.jpg" width="100%" style="margin: 0 auto"></p>

<p>Name your app “SeeFood” and click next. Select the folder where you wish to create your new project and click Create.
<img src="./How to train your own model for CoreML_files/44.jpg" width="100%" style="margin: 0 auto"></p>

<p>Once you’ve created your project, right click on the SeeFood folder under Project Navigator on the left and select <em>Add files to ‘SeeFood’…</em>
<img src="./How to train your own model for CoreML_files/45.jpg" width="100%" style="margin: 0 auto"></p>

<p>Find and select <code class="highlighter-rouge">food.mlmodel</code> from the folder where we ran <code class="highlighter-rouge">run.py</code>. Make sure <em>Copy items if needed</em> under <em>Options</em> is selected and click add.
<img src="./How to train your own model for CoreML_files/46.jpg" width="100%" style="margin: 0 auto"></p>

<p>After a few second <code class="highlighter-rouge">food.mlmodel</code> should appear under your Project Navigator. Click on the file and you should see the models detail.</p>

<p>Make sure under inputs you see <code class="highlighter-rouge">data Image&lt;BGR 227,227&gt;</code> and under outputs you see <code class="highlighter-rouge">prob Dictionary&lt;String,Double&gt;</code> and <code class="highlighter-rouge">classLabel String</code>. If you see anything else, remove the model and insert it again. If that doesn’t work, check the <code class="highlighter-rouge">run.py</code> we created earlier and make sure it is exactly as what’s used in this example and that it has access to all the files it’s referencing. 
<img src="./How to train your own model for CoreML_files/47.jpg" width="100%" style="margin: 0 auto"></p>

<p>We’re now ready to work with our CoreML model. Open <code class="highlighter-rouge">ViewController</code> and add the following before the class definition
<code class="highlighter-rouge">swift
import UIKit
import CoreML
import Vision
</code></p>

<p>The add the following function at the bottom of <code class="highlighter-rouge">ViewController</code></p>

<div class="highlighter-rouge"><pre class="highlight"><code>  <span class="kd">func</span> <span class="nf">detectScene</span><span class="p">(</span><span class="nv">image</span><span class="p">:</span> <span class="kt">CIImage</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">guard</span> <span class="k">let</span> <span class="nv">model</span> <span class="o">=</span> <span class="k">try</span><span class="p">?</span> <span class="kt">VNCoreMLModel</span><span class="p">(</span><span class="nv">for</span><span class="p">:</span> <span class="nf">food</span><span class="p">()</span><span class="o">.</span><span class="n">model</span><span class="p">)</span> <span class="k">else</span> <span class="p">{</span>
      <span class="nf">fatalError</span><span class="p">()</span>
    <span class="p">}</span>
    <span class="c1">// Create a Vision request with completion handler</span>
    <span class="k">let</span> <span class="nv">request</span> <span class="o">=</span> <span class="kt">VNCoreMLRequest</span><span class="p">(</span><span class="nv">model</span><span class="p">:</span> <span class="n">model</span><span class="p">)</span> <span class="p">{</span> <span class="p">[</span><span class="k">unowned</span> <span class="k">self</span><span class="p">]</span> <span class="n">request</span><span class="p">,</span> <span class="n">error</span> <span class="k">in</span>
      <span class="k">guard</span> <span class="k">let</span> <span class="nv">results</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">results</span> <span class="k">as?</span> <span class="p">[</span><span class="kt">VNClassificationObservation</span><span class="p">]</span> <span class="k">else</span> <span class="p">{</span>
          <span class="k">return</span>
      <span class="p">}</span>
      
      <span class="kt">DispatchQueue</span><span class="o">.</span><span class="n">main</span><span class="o">.</span><span class="k">as</span><span class="n">ync</span> <span class="p">{</span> <span class="p">[</span><span class="k">unowned</span> <span class="k">self</span><span class="p">]</span> <span class="k">in</span>
        <span class="n">results</span><span class="o">.</span><span class="nf">forEach</span><span class="p">({</span> <span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="k">in</span>
          <span class="k">if</span> <span class="kt">Int</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">confidence</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="p">{</span>
            <span class="k">self</span><span class="o">.</span><span class="n">settingImage</span> <span class="o">=</span> <span class="kc">false</span>
            <span class="nf">print</span><span class="p">(</span><span class="s">"</span><span class="se">\(</span><span class="kt">Int</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">confidence</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span><span class="se">)</span><span class="s">% it's </span><span class="se">\(</span><span class="n">result</span><span class="o">.</span><span class="n">identifier</span><span class="se">)</span><span class="s"> "</span><span class="p">)</span>
          <span class="p">}</span>
        <span class="p">})</span>
        <span class="nf">print</span><span class="p">(</span><span class="s">"********************************"</span><span class="p">)</span>
        
      <span class="p">}</span>
    <span class="p">}</span>
    <span class="k">let</span> <span class="nv">handler</span> <span class="o">=</span> <span class="kt">VNImageRequestHandler</span><span class="p">(</span><span class="nv">ciImage</span><span class="p">:</span> <span class="n">image</span><span class="p">)</span>
    <span class="kt">DispatchQueue</span><span class="o">.</span><span class="nf">global</span><span class="p">(</span><span class="nv">qos</span><span class="p">:</span> <span class="o">.</span><span class="n">userInitiated</span><span class="p">)</span><span class="o">.</span><span class="k">as</span><span class="n">ync</span> <span class="p">{</span>
      <span class="k">do</span> <span class="p">{</span>
        <span class="k">try</span> <span class="n">handler</span><span class="o">.</span><span class="nf">perform</span><span class="p">([</span><span class="n">request</span><span class="p">])</span>
      <span class="p">}</span> <span class="k">catch</span> <span class="p">{</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
</code></pre>
</div>

<p>To test our CoreML model, add some sample images that were not part of the training set to our <code class="highlighter-rouge">Assets.xcassets</code> folder
<img src="./How to train your own model for CoreML_files/50.jpg" width="100%" style="margin: 0 auto"></p>

<p>Go back to the <code class="highlighter-rouge">viewController</code> and add the following line in the <code class="highlighter-rouge">viewDidLoad</code> method after <code class="highlighter-rouge">super.viewDidLoad()</code></p>

<div class="highlighter-rouge"><pre class="highlight"><code>    <span class="k">if</span> <span class="k">let</span> <span class="nv">uiExamle</span> <span class="o">=</span> <span class="kt">UIImage</span><span class="p">(</span><span class="nv">named</span><span class="p">:</span><span class="s">"pizza"</span><span class="p">),</span>
      <span class="k">let</span> <span class="nv">example</span> <span class="o">=</span> <span class="kt">CIImage</span><span class="p">(</span><span class="nv">image</span><span class="p">:</span> <span class="n">uiExamle</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">self</span><span class="o">.</span><span class="nf">detectScene</span><span class="p">(</span><span class="nv">image</span><span class="p">:</span> <span class="n">example</span><span class="p">)</span>
    <span class="p">}</span>
</code></pre>
</div>

<p>change <code class="highlighter-rouge">pizza</code> to the name of whatever image file you added to <code class="highlighter-rouge">Assets.xcassets</code>. Build and run the app. Ignore the Simulator and take a look at the output in the console.</p>

<p><img src="./How to train your own model for CoreML_files/51.jpg" width="100%" style="margin: 0 auto"></p>

<p>Great! our CoreML model is working. However classifying an image from <code class="highlighter-rouge">Assets.xcassets</code> is not super useful.  Lets build out the app so it continuously takes a frame from the camera, runs it through our classifier and displays on the screen what it thinks it sees. CoreML is pretty fast and this makes for a much better user experience than having to take a picture and then run it through the classifier.</p>

<p>Click on <code class="highlighter-rouge">Main.storyboard</code>. Add a <code class="highlighter-rouge">UIImageView</code> and a <code class="highlighter-rouge">UILabel</code> to  the <code class="highlighter-rouge">ViewController</code> and link it back to an outlet on <code class="highlighter-rouge">ViewController.swift</code></p>

<p><img src="./How to train your own model for CoreML_files/48.jpg" width="100%" style="margin: 0 auto"></p>

<div class="highlighter-rouge"><pre class="highlight"><code>  <span class="kd">@IBOutlet</span> <span class="k">weak</span> <span class="k">var</span> <span class="nv">previewImage</span><span class="p">:</span> <span class="kt">UIImageView</span><span class="o">!</span>
  <span class="kd">@IBOutlet</span> <span class="k">weak</span> <span class="k">var</span> <span class="nv">iSee</span><span class="p">:</span> <span class="kt">UILabel</span><span class="o">!</span>
</code></pre>
</div>

<p>To capture individual frames from the camera we’re going to use the <code class="highlighter-rouge">FrameExtractor</code>.  A class described <a href="https://medium.com/ios-os-x-development/ios-camera-frames-extraction-d2c0f80ed05a">here</a> by <a href="https://medium.com/@borisohayon">Boris Ohayon</a>. The original classes is written in Swift 3, I have made the changes necessary and converted it to Swift 4 so you can copy past it directly from here. I do suggest going through it to understand how AVFoundation works. I’m not going to get into too much details since it’s outside the scope of CoreML and this guide but AVFoundation is definitely worth exploring. If you wish to dive into it, this is <a href="https://developer.apple.com/av-foundation/">a good place to start</a></p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kd">import</span> <span class="kt">UIKit</span>
<span class="kd">import</span> <span class="kt">AVFoundation</span>

<span class="kd">protocol</span> <span class="kt">FrameExtractorDelegate</span><span class="p">:</span> <span class="kd">class</span> <span class="p">{</span>
  <span class="kd">func</span> <span class="nf">captured</span><span class="p">(</span><span class="nv">image</span><span class="p">:</span> <span class="kt">UIImage</span><span class="p">)</span>
<span class="p">}</span>

<span class="kd">class</span> <span class="kt">FrameExtractor</span><span class="p">:</span> <span class="kt">NSObject</span><span class="p">,</span> <span class="kt">AVCaptureVideoDataOutputSampleBufferDelegate</span> <span class="p">{</span>
  
  <span class="kd">private</span> <span class="k">var</span> <span class="nv">position</span> <span class="o">=</span> <span class="kt">AVCaptureDevice</span><span class="o">.</span><span class="kt">Position</span><span class="o">.</span><span class="n">back</span>
  <span class="kd">private</span> <span class="k">let</span> <span class="nv">quality</span> <span class="o">=</span> <span class="kt">AVCaptureSession</span><span class="o">.</span><span class="kt">Preset</span><span class="o">.</span><span class="n">medium</span>
  
  <span class="kd">private</span> <span class="k">var</span> <span class="nv">permissionGranted</span> <span class="o">=</span> <span class="kc">false</span>
  <span class="kd">private</span> <span class="k">let</span> <span class="nv">sessionQueue</span> <span class="o">=</span> <span class="kt">DispatchQueue</span><span class="p">(</span><span class="nv">label</span><span class="p">:</span> <span class="s">"session queue"</span><span class="p">)</span>
  <span class="kd">private</span> <span class="k">let</span> <span class="nv">captureSession</span> <span class="o">=</span> <span class="kt">AVCaptureSession</span><span class="p">()</span>
  <span class="kd">private</span> <span class="k">let</span> <span class="nv">context</span> <span class="o">=</span> <span class="kt">CIContext</span><span class="p">()</span>
  
  <span class="k">weak</span> <span class="k">var</span> <span class="nv">delegate</span><span class="p">:</span> <span class="kt">FrameExtractorDelegate</span><span class="p">?</span>
  
  <span class="k">override</span> <span class="nf">init</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">super</span><span class="o">.</span><span class="nf">init</span><span class="p">()</span>
    <span class="nf">checkPermission</span><span class="p">()</span>
    <span class="n">sessionQueue</span><span class="o">.</span><span class="k">as</span><span class="n">ync</span> <span class="p">{</span> <span class="p">[</span><span class="k">unowned</span> <span class="k">self</span><span class="p">]</span> <span class="k">in</span>
      <span class="k">self</span><span class="o">.</span><span class="nf">configureSession</span><span class="p">()</span>
      <span class="k">self</span><span class="o">.</span><span class="n">captureSession</span><span class="o">.</span><span class="nf">startRunning</span><span class="p">()</span>
    <span class="p">}</span>
  <span class="p">}</span>
  
  <span class="kd">public</span> <span class="kd">func</span> <span class="nf">flipCamera</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">sessionQueue</span><span class="o">.</span><span class="k">as</span><span class="n">ync</span> <span class="p">{</span> <span class="p">[</span><span class="k">unowned</span> <span class="k">self</span><span class="p">]</span> <span class="k">in</span>
      <span class="k">self</span><span class="o">.</span><span class="n">captureSession</span><span class="o">.</span><span class="nf">beginConfiguration</span><span class="p">()</span>
      <span class="k">guard</span> <span class="k">let</span> <span class="nv">currentCaptureInput</span> <span class="o">=</span> <span class="k">self</span><span class="o">.</span><span class="n">captureSession</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">first</span> <span class="k">else</span> <span class="p">{</span> <span class="k">return</span> <span class="p">}</span>
      <span class="k">self</span><span class="o">.</span><span class="n">captureSession</span><span class="o">.</span><span class="nf">removeInput</span><span class="p">(</span><span class="n">currentCaptureInput</span><span class="p">)</span>
      <span class="k">guard</span> <span class="k">let</span> <span class="nv">currentCaptureOutput</span> <span class="o">=</span> <span class="k">self</span><span class="o">.</span><span class="n">captureSession</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">first</span> <span class="k">else</span> <span class="p">{</span> <span class="k">return</span> <span class="p">}</span>
      <span class="k">self</span><span class="o">.</span><span class="n">captureSession</span><span class="o">.</span><span class="nf">removeOutput</span><span class="p">(</span><span class="n">currentCaptureOutput</span><span class="p">)</span>
      <span class="k">self</span><span class="o">.</span><span class="n">position</span> <span class="o">=</span> <span class="k">self</span><span class="o">.</span><span class="n">position</span> <span class="o">==</span> <span class="o">.</span><span class="n">front</span> <span class="p">?</span> <span class="o">.</span><span class="nv">back</span> <span class="p">:</span> <span class="o">.</span><span class="n">front</span>
      <span class="k">self</span><span class="o">.</span><span class="nf">configureSession</span><span class="p">()</span>
      <span class="k">self</span><span class="o">.</span><span class="n">captureSession</span><span class="o">.</span><span class="nf">commitConfiguration</span><span class="p">()</span>
    <span class="p">}</span>
  <span class="p">}</span>
  
  <span class="c1">// MARK: AVSession configuration</span>
  <span class="kd">private</span> <span class="kd">func</span> <span class="nf">checkPermission</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">switch</span> <span class="kt">AVCaptureDevice</span><span class="o">.</span><span class="nf">authorizationStatus</span><span class="p">(</span><span class="nv">for</span><span class="p">:</span> <span class="kt">AVMediaType</span><span class="o">.</span><span class="n">video</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">case</span> <span class="o">.</span><span class="nv">authorized</span><span class="p">:</span>
      <span class="n">permissionGranted</span> <span class="o">=</span> <span class="kc">true</span>
    <span class="k">case</span> <span class="o">.</span><span class="nv">notDetermined</span><span class="p">:</span>
      <span class="nf">requestPermission</span><span class="p">()</span>
    <span class="k">default</span><span class="p">:</span>
      <span class="n">permissionGranted</span> <span class="o">=</span> <span class="kc">false</span>
    <span class="p">}</span>
  <span class="p">}</span>
  
  <span class="kd">private</span> <span class="kd">func</span> <span class="nf">requestPermission</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">sessionQueue</span><span class="o">.</span><span class="nf">suspend</span><span class="p">()</span>
    <span class="kt">AVCaptureDevice</span><span class="o">.</span><span class="nf">requestAccess</span><span class="p">(</span><span class="nv">for</span><span class="p">:</span> <span class="kt">AVMediaType</span><span class="o">.</span><span class="n">video</span><span class="p">)</span> <span class="p">{</span> <span class="p">[</span><span class="k">unowned</span> <span class="k">self</span><span class="p">]</span> <span class="n">granted</span> <span class="k">in</span>
      <span class="k">self</span><span class="o">.</span><span class="n">permissionGranted</span> <span class="o">=</span> <span class="n">granted</span>
      <span class="k">self</span><span class="o">.</span><span class="n">sessionQueue</span><span class="o">.</span><span class="nf">resume</span><span class="p">()</span>
    <span class="p">}</span>
  <span class="p">}</span>
  
  <span class="kd">private</span> <span class="kd">func</span> <span class="nf">configureSession</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">guard</span> <span class="n">permissionGranted</span> <span class="k">else</span> <span class="p">{</span> <span class="k">return</span> <span class="p">}</span>
    <span class="n">captureSession</span><span class="o">.</span><span class="n">sessionPreset</span> <span class="o">=</span> <span class="n">quality</span>
    <span class="k">guard</span> <span class="k">let</span> <span class="nv">captureDevice</span> <span class="o">=</span> <span class="nf">selectCaptureDevice</span><span class="p">()</span> <span class="k">else</span> <span class="p">{</span> <span class="k">return</span> <span class="p">}</span>
    <span class="k">guard</span> <span class="k">let</span> <span class="nv">captureDeviceInput</span> <span class="o">=</span> <span class="k">try</span><span class="p">?</span> <span class="kt">AVCaptureDeviceInput</span><span class="p">(</span><span class="nv">device</span><span class="p">:</span> <span class="n">captureDevice</span><span class="p">)</span> <span class="k">else</span> <span class="p">{</span> <span class="k">return</span> <span class="p">}</span>
    <span class="k">guard</span> <span class="n">captureSession</span><span class="o">.</span><span class="nf">canAddInput</span><span class="p">(</span><span class="n">captureDeviceInput</span><span class="p">)</span> <span class="k">else</span> <span class="p">{</span> <span class="k">return</span> <span class="p">}</span>
    <span class="n">captureSession</span><span class="o">.</span><span class="nf">addInput</span><span class="p">(</span><span class="n">captureDeviceInput</span><span class="p">)</span>
    <span class="k">let</span> <span class="nv">videoOutput</span> <span class="o">=</span> <span class="kt">AVCaptureVideoDataOutput</span><span class="p">()</span>
    <span class="n">videoOutput</span><span class="o">.</span><span class="nf">setSampleBufferDelegate</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="nv">queue</span><span class="p">:</span> <span class="kt">DispatchQueue</span><span class="p">(</span><span class="nv">label</span><span class="p">:</span> <span class="s">"sample buffer"</span><span class="p">))</span>
    <span class="k">guard</span> <span class="n">captureSession</span><span class="o">.</span><span class="nf">canAddOutput</span><span class="p">(</span><span class="n">videoOutput</span><span class="p">)</span> <span class="k">else</span> <span class="p">{</span> <span class="k">return</span> <span class="p">}</span>
    <span class="n">captureSession</span><span class="o">.</span><span class="nf">addOutput</span><span class="p">(</span><span class="n">videoOutput</span><span class="p">)</span>
    <span class="k">guard</span> <span class="k">let</span> <span class="nv">connection</span> <span class="o">=</span> <span class="n">videoOutput</span><span class="o">.</span><span class="nf">connection</span><span class="p">(</span><span class="nv">with</span><span class="p">:</span> <span class="kt">AVFoundation</span><span class="o">.</span><span class="kt">AVMediaType</span><span class="o">.</span><span class="n">video</span><span class="p">)</span> <span class="k">else</span> <span class="p">{</span> <span class="k">return</span> <span class="p">}</span>
    <span class="k">guard</span> <span class="n">connection</span><span class="o">.</span><span class="n">isVideoOrientationSupported</span> <span class="k">else</span> <span class="p">{</span> <span class="k">return</span> <span class="p">}</span>
    <span class="k">guard</span> <span class="n">connection</span><span class="o">.</span><span class="n">isVideoMirroringSupported</span> <span class="k">else</span> <span class="p">{</span> <span class="k">return</span> <span class="p">}</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">videoOrientation</span> <span class="o">=</span> <span class="o">.</span><span class="n">portrait</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">isVideoMirrored</span> <span class="o">=</span> <span class="n">position</span> <span class="o">==</span> <span class="o">.</span><span class="n">front</span>
  <span class="p">}</span>
  
  <span class="kd">private</span> <span class="kd">func</span> <span class="nf">selectCaptureDevice</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kt">AVCaptureDevice</span><span class="p">?</span> <span class="p">{</span>
    <span class="k">return</span> <span class="kt">AVCaptureDevice</span><span class="o">.</span><span class="nf">default</span><span class="p">(</span><span class="o">.</span><span class="n">builtInWideAngleCamera</span><span class="p">,</span> <span class="nv">for</span><span class="p">:</span> <span class="kt">AVMediaType</span><span class="o">.</span><span class="n">video</span><span class="p">,</span> <span class="nv">position</span><span class="p">:</span> <span class="o">.</span><span class="n">back</span><span class="p">)</span>
  <span class="p">}</span>
  
  <span class="c1">// MARK: Sample buffer to UIImage conversion</span>
  <span class="kd">private</span> <span class="kd">func</span> <span class="nf">imageFromSampleBuffer</span><span class="p">(</span><span class="nv">sampleBuffer</span><span class="p">:</span> <span class="kt">CMSampleBuffer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kt">UIImage</span><span class="p">?</span> <span class="p">{</span>
    <span class="k">guard</span> <span class="k">let</span> <span class="nv">imageBuffer</span> <span class="o">=</span> <span class="kt">CMSampleBufferGetImageBuffer</span><span class="p">(</span><span class="n">sampleBuffer</span><span class="p">)</span> <span class="k">else</span> <span class="p">{</span> <span class="k">return</span> <span class="kc">nil</span> <span class="p">}</span>
    <span class="k">let</span> <span class="nv">ciImage</span> <span class="o">=</span> <span class="kt">CIImage</span><span class="p">(</span><span class="nv">cvPixelBuffer</span><span class="p">:</span> <span class="n">imageBuffer</span><span class="p">)</span>
    <span class="k">guard</span> <span class="k">let</span> <span class="nv">cgImage</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="nf">createCGImage</span><span class="p">(</span><span class="n">ciImage</span><span class="p">,</span> <span class="nv">from</span><span class="p">:</span> <span class="n">ciImage</span><span class="o">.</span><span class="n">extent</span><span class="p">)</span> <span class="k">else</span> <span class="p">{</span> <span class="k">return</span> <span class="kc">nil</span> <span class="p">}</span>
    <span class="k">return</span> <span class="kt">UIImage</span><span class="p">(</span><span class="nv">cgImage</span><span class="p">:</span> <span class="n">cgImage</span><span class="p">)</span>
  <span class="p">}</span>
  
  <span class="c1">// MARK: AVCaptureVideoDataOutputSampleBufferDelegate</span>
  <span class="kd">func</span> <span class="nf">captureOutput</span><span class="p">(</span><span class="n">_</span> <span class="nv">captureOutput</span><span class="p">:</span> <span class="kt">AVCaptureOutput</span><span class="p">,</span> <span class="n">didOutput</span> <span class="nv">sampleBuffer</span><span class="p">:</span> <span class="kt">CMSampleBuffer</span><span class="p">,</span> <span class="n">from</span> <span class="nv">connection</span><span class="p">:</span> <span class="kt">AVCaptureConnection</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">guard</span> <span class="k">let</span> <span class="nv">uiImage</span> <span class="o">=</span> <span class="nf">imageFromSampleBuffer</span><span class="p">(</span><span class="nv">sampleBuffer</span><span class="p">:</span> <span class="n">sampleBuffer</span><span class="p">)</span> <span class="k">else</span> <span class="p">{</span> <span class="k">return</span> <span class="p">}</span>
    <span class="kt">DispatchQueue</span><span class="o">.</span><span class="n">main</span><span class="o">.</span><span class="k">as</span><span class="n">ync</span> <span class="p">{</span> <span class="p">[</span><span class="k">unowned</span> <span class="k">self</span><span class="p">]</span> <span class="k">in</span>
      <span class="k">self</span><span class="o">.</span><span class="n">delegate</span><span class="p">?</span><span class="o">.</span><span class="nf">captured</span><span class="p">(</span><span class="nv">image</span><span class="p">:</span> <span class="n">uiImage</span><span class="p">)</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre>
</div>

<p>Go back to <code class="highlighter-rouge">ViewController</code> . To get an image, classify it and display the top prediction on the screen, change the <code class="highlighter-rouge">ViewController</code> so it looks like this:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kd">import</span> <span class="kt">UIKit</span>
<span class="kd">import</span> <span class="kt">CoreML</span>
<span class="kd">import</span> <span class="kt">Vision</span>
<span class="kd">import</span> <span class="kt">AVFoundation</span>

<span class="kd">class</span> <span class="kt">ViewController</span><span class="p">:</span> <span class="kt">UIViewController</span><span class="p">,</span> <span class="kt">AVCapturePhotoCaptureDelegate</span><span class="p">,</span> <span class="kt">FrameExtractorDelegate</span> <span class="p">{</span>
 
  <span class="k">var</span> <span class="nv">frameExtractor</span><span class="p">:</span> <span class="kt">FrameExtractor</span><span class="o">!</span>
  
  <span class="kd">@IBOutlet</span> <span class="k">weak</span> <span class="k">var</span> <span class="nv">previewImage</span><span class="p">:</span> <span class="kt">UIImageView</span><span class="o">!</span>
  <span class="kd">@IBOutlet</span> <span class="k">weak</span> <span class="k">var</span> <span class="nv">iSee</span><span class="p">:</span> <span class="kt">UILabel</span><span class="o">!</span>

  <span class="k">var</span> <span class="nv">settingImage</span> <span class="o">=</span> <span class="kc">false</span>
  
  <span class="k">var</span> <span class="nv">currentImage</span><span class="p">:</span> <span class="kt">CIImage</span><span class="p">?</span> <span class="p">{</span>
    <span class="k">didSet</span> <span class="p">{</span>
      <span class="k">if</span> <span class="k">let</span> <span class="nv">image</span> <span class="o">=</span> <span class="n">currentImage</span><span class="p">{</span>
        <span class="k">self</span><span class="o">.</span><span class="nf">detectScene</span><span class="p">(</span><span class="nv">image</span><span class="p">:</span> <span class="n">image</span><span class="p">)</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
  
  <span class="k">override</span> <span class="kd">func</span> <span class="nf">viewDidLoad</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">super</span><span class="o">.</span><span class="nf">viewDidLoad</span><span class="p">()</span>
    <span class="n">frameExtractor</span> <span class="o">=</span> <span class="kt">FrameExtractor</span><span class="p">()</span>
    <span class="n">frameExtractor</span><span class="o">.</span><span class="n">delegate</span> <span class="o">=</span> <span class="k">self</span>
  <span class="p">}</span>
  
  <span class="kd">func</span> <span class="nf">captured</span><span class="p">(</span><span class="nv">image</span><span class="p">:</span> <span class="kt">UIImage</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">self</span><span class="o">.</span><span class="n">previewImage</span><span class="o">.</span><span class="n">image</span> <span class="o">=</span> <span class="n">image</span>
    <span class="k">if</span> <span class="k">let</span> <span class="nv">cgImage</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">cgImage</span><span class="p">,</span> <span class="o">!</span><span class="n">settingImage</span> <span class="p">{</span>
      <span class="n">settingImage</span> <span class="o">=</span> <span class="kc">true</span>
      <span class="kt">DispatchQueue</span><span class="o">.</span><span class="nf">global</span><span class="p">(</span><span class="nv">qos</span><span class="p">:</span> <span class="o">.</span><span class="n">userInteractive</span><span class="p">)</span><span class="o">.</span><span class="k">as</span><span class="n">ync</span> <span class="p">{[</span><span class="k">unowned</span> <span class="k">self</span><span class="p">]</span> <span class="k">in</span>
        <span class="k">self</span><span class="o">.</span><span class="n">currentImage</span> <span class="o">=</span> <span class="kt">CIImage</span><span class="p">(</span><span class="nv">cgImage</span><span class="p">:</span> <span class="n">cgImage</span><span class="p">)</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="kd">func</span> <span class="nf">addEmoji</span><span class="p">(</span><span class="nv">id</span><span class="p">:</span> <span class="kt">String</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kt">String</span> <span class="p">{</span>
    <span class="k">switch</span> <span class="n">id</span> <span class="p">{</span>
    <span class="k">case</span> <span class="s">"pizza"</span><span class="p">:</span>
      <span class="k">return</span> <span class="s">"🍕"</span>
    <span class="k">case</span> <span class="s">"hot dog"</span><span class="p">:</span>
      <span class="k">return</span> <span class="s">"🌭"</span>
    <span class="k">case</span> <span class="s">"chicken wings"</span><span class="p">:</span>
      <span class="k">return</span> <span class="s">"🍗"</span>
    <span class="k">case</span> <span class="s">"french fries"</span><span class="p">:</span>
      <span class="k">return</span> <span class="s">"🍟"</span>
    <span class="k">case</span> <span class="s">"sushi"</span><span class="p">:</span>
      <span class="k">return</span> <span class="s">"🍣"</span>
    <span class="k">case</span> <span class="s">"chocolate cake"</span><span class="p">:</span>
      <span class="k">return</span> <span class="s">"🍫🍰"</span>
    <span class="k">case</span> <span class="s">"donut"</span><span class="p">:</span>
      <span class="k">return</span> <span class="s">"🍩"</span>
    <span class="k">case</span> <span class="s">"spaghetti bolognese"</span><span class="p">:</span>
      <span class="k">return</span> <span class="s">"🍝"</span>
    <span class="k">case</span> <span class="s">"caesar salad"</span><span class="p">:</span>
      <span class="k">return</span> <span class="s">"🥗"</span>
    <span class="k">case</span> <span class="s">"macaroni and cheese"</span><span class="p">:</span>
      <span class="k">return</span> <span class="s">"🧀"</span>
    <span class="k">default</span><span class="p">:</span>
      <span class="k">return</span> <span class="s">""</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="kd">func</span> <span class="nf">detectScene</span><span class="p">(</span><span class="nv">image</span><span class="p">:</span> <span class="kt">CIImage</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">guard</span> <span class="k">let</span> <span class="nv">model</span> <span class="o">=</span> <span class="k">try</span><span class="p">?</span> <span class="kt">VNCoreMLModel</span><span class="p">(</span><span class="nv">for</span><span class="p">:</span> <span class="nf">food</span><span class="p">()</span><span class="o">.</span><span class="n">model</span><span class="p">)</span> <span class="k">else</span> <span class="p">{</span>
      <span class="nf">fatalError</span><span class="p">()</span>
    <span class="p">}</span>
    <span class="c1">// Create a Vision request with completion handler</span>
    <span class="k">let</span> <span class="nv">request</span> <span class="o">=</span> <span class="kt">VNCoreMLRequest</span><span class="p">(</span><span class="nv">model</span><span class="p">:</span> <span class="n">model</span><span class="p">)</span> <span class="p">{</span> <span class="p">[</span><span class="k">unowned</span> <span class="k">self</span><span class="p">]</span> <span class="n">request</span><span class="p">,</span> <span class="n">error</span> <span class="k">in</span>
      <span class="k">guard</span> <span class="k">let</span> <span class="nv">results</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">results</span> <span class="k">as?</span> <span class="p">[</span><span class="kt">VNClassificationObservation</span><span class="p">],</span>
        <span class="k">let</span> <span class="nv">_</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">first</span> <span class="k">else</span> <span class="p">{</span>
          <span class="k">self</span><span class="o">.</span><span class="n">settingImage</span> <span class="o">=</span> <span class="kc">false</span>
          <span class="k">return</span>
      <span class="p">}</span>
      
      <span class="kt">DispatchQueue</span><span class="o">.</span><span class="n">main</span><span class="o">.</span><span class="k">as</span><span class="n">ync</span> <span class="p">{</span> <span class="p">[</span><span class="k">unowned</span> <span class="k">self</span><span class="p">]</span> <span class="k">in</span>
        <span class="k">if</span> <span class="k">let</span> <span class="nv">first</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">first</span> <span class="p">{</span>
           <span class="k">if</span> <span class="kt">Int</span><span class="p">(</span><span class="n">first</span><span class="o">.</span><span class="n">confidence</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="p">{</span>
            <span class="k">self</span><span class="o">.</span><span class="n">iSee</span><span class="o">.</span><span class="n">text</span> <span class="o">=</span> <span class="s">"I see </span><span class="se">\(</span><span class="n">first</span><span class="o">.</span><span class="n">identifier</span><span class="se">)</span><span class="s"> </span><span class="se">\(</span><span class="k">self</span><span class="o">.</span><span class="nf">addEmoji</span><span class="p">(</span><span class="nv">id</span><span class="p">:</span> <span class="n">first</span><span class="o">.</span><span class="n">identifier</span><span class="p">)</span><span class="se">)</span><span class="s">"</span>
            <span class="k">self</span><span class="o">.</span><span class="n">settingImage</span> <span class="o">=</span> <span class="kc">false</span>
          <span class="p">}</span>
        <span class="p">}</span>
<span class="c1">//        results.forEach({ (result) in</span>
<span class="c1">//          if Int(result.confidence * 100) &gt; 1 {</span>
<span class="c1">//            self.settingImage = false</span>
<span class="c1">//            print("\(Int(result.confidence * 100))% it's \(result.identifier) ")</span>
<span class="c1">//          }</span>
<span class="c1">//        })</span>
       <span class="c1">// print("********************************")</span>
        
      <span class="p">}</span>
    <span class="p">}</span>
    <span class="k">let</span> <span class="nv">handler</span> <span class="o">=</span> <span class="kt">VNImageRequestHandler</span><span class="p">(</span><span class="nv">ciImage</span><span class="p">:</span> <span class="n">image</span><span class="p">)</span>
    <span class="kt">DispatchQueue</span><span class="o">.</span><span class="nf">global</span><span class="p">(</span><span class="nv">qos</span><span class="p">:</span> <span class="o">.</span><span class="n">userInteractive</span><span class="p">)</span><span class="o">.</span><span class="k">as</span><span class="n">ync</span> <span class="p">{</span>
      <span class="k">do</span> <span class="p">{</span>
        <span class="k">try</span> <span class="n">handler</span><span class="o">.</span><span class="nf">perform</span><span class="p">([</span><span class="n">request</span><span class="p">])</span>
      <span class="p">}</span> <span class="k">catch</span> <span class="p">{</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>

</code></pre>
</div>

<p>The code above is fairly straightforward: we set our <code class="highlighter-rouge">ViewController</code> to conform to <code class="highlighter-rouge">FrameExtractorDelegate</code>. We create an instance of <code class="highlighter-rouge">FrameExtractor</code> named <code class="highlighter-rouge">frameExtractor</code>. We set its <code class="highlighter-rouge">delegate</code> to <code class="highlighter-rouge">self</code>and implement <code class="highlighter-rouge">func captured(image: UIImage)</code> to complete the delegation implementation.</p>

<p>We declare a <code class="highlighter-rouge">CIImage</code> variable named <code class="highlighter-rouge">currentImage</code> and set its value whenever <code class="highlighter-rouge">captured</code> returns an image. We add a <code class="highlighter-rouge">didSet</code> to the <code class="highlighter-rouge">currentImage</code> to observe when its value changes and call <code class="highlighter-rouge">detectScene</code> with the new image. Since <code class="highlighter-rouge">captured</code> will take less time than <code class="highlighter-rouge">detectScene</code>, to prevent continuous calls into <code class="highlighter-rouge">detectScene</code> before it’s done we set a boolean flag called <code class="highlighter-rouge">settingImage</code>. This flag is set to true when a new image has been captured and set to false when <code class="highlighter-rouge">detectScene</code> has classified it. If it’s true we skip the image captured.</p>

<p>Build and deploy the app on a device running iOS 11. The first time the app runs it will ask for a permission to use camera. If you’ve been following this guide so far, your app will most likely crash. The error you will see is <code class="highlighter-rouge">The app’s Info.plist must contain an NSCameraUsageDescription key.</code> error. Since iOS 10 you need to provide a usage description which is shown to the user when the popup for a specific permission is displayed. To fix this,  a string value describing your reason needs to be added to the <code class="highlighter-rouge">info.plist</code> file for <code class="highlighter-rouge">NSCameraUsageDescription</code> key.</p>

<p><img src="./How to train your own model for CoreML_files/52.jpg" width="100%" style="margin: 0 auto"></p>

<p>Build and run. The app should launch, once the permission is given to use the camera, you should see what the camera is seeing and the label should update to what CoreML thinks is in front of it.</p>

<div style="margin: 0 auto;width:320px;">
<video width="100%" autoplay="" controls="" align="center">
  <source src="https://s3.amazonaws.com/pixpit/See+Food-+Detect+various+dishes+using+CoreML.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</div>

<p>Congratulation, you just trained and integrated your own image classifier and  deployed it to an iOS device!  As lengthy and convoluted the process might appear, once you go through it you’d realize it’s simple. Thanks to Caffe, DIGITS and now CoreML the hard parts have been figured out and it’s up to you to collect your dataset, train your models and build awesome apps. The amount of coding is minimal and the power is immense. Machine learning is the future and I would love to see what you do with it. Feel free to hit me up on <a href="https://twitter.com/kingreza">twitter</a> and show me your creations!</p>

<p><a href="https://github.com/kingreza/SeeFood/">Click here to view repo for the full project</a></p>

<h3> Glossary </h3>
<ul>
<li><a name="alexnet"></a>
<a href="http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf">AlexNet</a>: a convolutional neural network which competed in the ImageNet Large Scale Visual Recognition Challenge in 2012. AlexNet was designed by Alex Krizhevsky.</li>
<li><a name="anaconda"></a>
<a href="https://docs.continuum.io/anaconda/navigator/">Anaconda Navigator</a>: a desktop graphical user interface (GUI) that is included in Anaconda® and allows you to launch applications and easily manage conda packages, environments and channels without using command-line commands.</li>
<li><a name="caffe"></a>
<a href="http://caffe.berkeleyvision.org/">Caffe</a>: a deep learning framework made with expression, speed, and modularity in mind. It is developed by [Berkeley AI Research](http://bair.berkeley.edu/)     </li>
<li><a name="coreml"></a>
<a href="https://developer.apple.com/documentation/coreml">CoreML</a>:[CoreML](https://developer.apple.com/documentation/coreml): Apple's machine learning framework. It can integrate machine learning models into your app.   </li>
<li><a name="coremltool"></a>
<a href="https://pypi.python.org/pypi/coremltools">coremltool</a>: a python package for creating, examining, and testing models in the .mlmodel format.</li>
<li><a name="digits"></a>
<a href="https://developer.nvidia.com/digits">DIGITS</a>: simplify common deep learning tasks such as managing data, designing and training neural networks on multi-GPU systems, monitoring performance in real time with advanced visualizations, and selecting the best performing model from the results browser for deployment. DIGITS is completely interactive so that data scientists can focus on designing and training networks rather than programming and debugging.    </li>
<li><a name="ec2"></a>
<a href="https://aws.amazon.com/ec2/">EC2</a>: Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers.    </li>
<li><a name="food101"></a>
<a href="https://www.vision.ee.ethz.ch/datasets_extra/food-101/">Food 101</a> Food-101 -- Mining Discriminative Components with Random Forests by Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc   </li>
</ul>

<h3> Further readings </h3>

<p>If you’ve made it this far, congratulations again. Although we’ve covered a lot,  we haven’t even scratched the surface. Here are some suggestions as to where to go from here:</p>

<p><a href="https://www.youtube.com/watch?v=7VGMGtDqgwI">WWDC 2017 - Introducing Core ML</a>   <br>
<a href="https://www.youtube.com/watch?v=cyWYTwDtbyw">WWDC 2017 - Core ML in depth</a>   <br>
<a href="https://www.youtube.com/watch?v=UXhgjUIXUak">WWDC 2017 - Vision Framework</a>    <br>
<a href="https://developer.apple.com/documentation/coreml">Apple CoreML Documentation</a>    <br>
<a href="http://ucb-icsi-vision-group.github.io/caffe-paper/caffe.pdf">Caffe: Convolutional Architecture for Fast Feature Embedding</a> Caffe paper   <br>
<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a> AlexNet paper    <br>
<a href="https://docs.google.com/presentation/d/1UeKXVgRvvxg9OUdh_UiC5G71UMscNPlvArsWER41PsU/edit#slide=id.p">DIY Deep Learning for Vision: A hands on tutorial with Caffe</a>
<a href="http://caffe.berkeleyvision.org/tutorial/">Caffe Tutorial</a>    <br>
<a href="https://devblogs.nvidia.com/parallelforall/deep-learning-computer-vision-caffe-cudnn/">Deep Learning for Computer Vision with Caffe and cuDNN</a>    <br>
<a href="http://karpathy.github.io/neuralnets/">Hacker’s guide to Neural Networks</a> Great article by Andrej Karpathy
<a href="https://www.youtube.com/watch?v=QZaAcl_F9R0">Deep Learning using NVIDIA DIGITS 3 on EC2</a> by  Satya Mallick.   <br>
<a href="http://machinethink.net/blog/">Matthijs Hollemans</a> Matthijs Hollemans’ blog on Machine Learning.</p>


  <!-- Disqus -->
  
  <div class="post-disqus">
      <section id="disqus_thread"><iframe id="dsq-app6317" name="dsq-app6317" allowtransparency="true" frameborder="0" scrolling="no" tabindex="0" title="Disqus" width="100%" src="./How to train your own model for CoreML_files/saved_resource.html" style="width: 1px !important; min-width: 100% !important; border: none !important; overflow: hidden !important; height: 403px !important;" horizontalscrolling="no" verticalscrolling="no"></iframe></section>
      <script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//shirazian.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the &lt;a href="https://disqus.com/?ref_noscript"&gt;comments powered by Disqus.&lt;/a&gt;</noscript>

  </div>
  

</div>


    <!-- Documents about icons are here: http://fontawesome.io/icons/ -->
<div class="footer">
  <hr>
    <div class="footer-link">
    
	
	
	
	

    
    <a href="https://twitter.com/kingreza"><i class="fa fa-twitter" aria-hidden="true"></i></a>
    

    
    <a href="https://github.com/kingreza"><i class="fa fa-github" aria-hidden="true"></i></a>
    
	
	
    <a href="http://stackoverflow.com/users/1327778"><i class="fa fa-stack-overflow" aria-hidden="true"></i></a>
    
	
	

    
    <a href="https://www.linkedin.com/in/shirazian"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
    
	
	
	
	
	
	
    <a href="https://www.instagram.com/king_reza"><i class="fa fa-instagram" aria-hidden="true"></i></a>
    
	
	

    
    <a href="https://medium.com/@kingreza"><i class="fa fa-medium" aria-hidden="true"></i></a>
    

    

    
    <a href="mailto:reza.shirazian@gmail.com"><i class="fa fa-envelope" aria-hidden="true"></i></a>
    

  </div>
  © 2017 Reza Shirazian. All rights reserved.
</div>

  </div>


<iframe style="display: none;" src="./How to train your own model for CoreML_files/saved_resource(1).html"></iframe></body></html>